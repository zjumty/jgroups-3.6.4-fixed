
Check list for performance tuning (mainly gige)
===============================================

Author: Bela Ban, Eric

For the transmit queue:

$ /sbin/ifconfig [eth0|eth1]

The txqueue size is currently 1000.

To get the kernel queue size for incoming packets:

$ sysctl -a | grep net.core.netdev_max_backlog

For socket buffer info:

$ sysctl -a | grep net.core
  net.core.divert_version = 0.46
  net.core.hot_list_length = 128
  net.core.optmem_max = 10240
  net.core.message_burst = 50
  net.core.message_cost = 5
  net.core.mod_cong = 290
  net.core.lo_cong = 100
  net.core.no_cong = 20
  net.core.no_cong_thresh = 20
  net.core.netdev_max_backlog = 300
  net.core.dev_weight = 64

** these are the default and max sizes for socket buffer memory

net.core.rmem_default = 65535
  net.core.wmem_default = 65535
  net.core.rmem_max = 131071
  net.core.wmem_max = 131071

Here are some other links:

Squeeze Your Gigabit NIC for Top Performance
http://www.enterpriseitplanet.com/networking/features/article.php/3497796

How to achieve Gigabit speeds with Linux
http://datatag.web.cern.ch/datatag/howto/tcp.html

Enabling High Performance Data Transfers
http://www.psc.edu/networking/projects/tcptune/


>> #2 What's the MTU (do you use jumbo frames, or the regular 1500b) ?


In general, we have not tuned this environment. This needs to be
addressed as part of the QA Lab upgrades we're targeting for next
month. I'll be creating some Jira tasks and working with Ryan's
team on this, so we should discuss the network needs along with
other system relates topics (oracle tuning, etc.).

Regular 1500 for now. We need to revisit this for some upcoming QA lab
updates were doing. If we need a dedicated GIGE switch for testing,
we need to get this in the budget (I think it's worth the 1.5k myself).
If we can't get a dedicated switch, we'll see if we can enable the
jumbo frames on the private side subnet (but this requires ALL connected
machines be gige w/ jumbo frame support - not an issue w/ the current
hw, but something to think about).
